{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T12:17:25.220616Z",
     "start_time": "2024-06-01T12:17:10.649731Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import selenium\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from time import sleep\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "driver.get(\"https://twitter.com/login\")\n",
    "sleep(3)\n",
    "# subject = \"saurabh kumar\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26e0b4843e8087d1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T12:17:30.162091Z",
     "start_time": "2024-06-01T12:17:26.784876Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sleep(3)\n",
    "username = driver.find_element(By.XPATH,\"//input[@name='text']\")\n",
    "username.send_keys(\"********************\")\n",
    "next_button = driver.find_element(By.XPATH,\"//span[contains(text(),'Next')]\")\n",
    "next_button.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c0f46880b69068",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T12:17:38.405524Z",
     "start_time": "2024-06-01T12:17:34.293643Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sleep(4)\n",
    "password = driver.find_element(By.XPATH,\"//input[@name='password']\")\n",
    "password.send_keys('*********************')\n",
    "log_in = driver.find_element(By.XPATH,\"//span[contains(text(),'Log in')]\")\n",
    "log_in.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0bce49b73a10bfa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T12:17:58.044657Z",
     "start_time": "2024-06-01T12:17:53.749406Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sleep(4)\n",
    "subject = \"virat kohali\"\n",
    "search_box = driver.find_element(By.XPATH,\"//input[@data-testid='SearchBox_Search_Input']\")\n",
    "search_box.send_keys(subject)\n",
    "search_box.send_keys(Keys.ENTER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "baefc7ac9cf812a3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T12:18:10.594135Z",
     "start_time": "2024-06-01T12:18:07.475750Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sleep(3)\n",
    "people = driver.find_element(By.XPATH,\"//span[contains(text(),'People')]\")\n",
    "people.click()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5c4247dd9fb1437",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T12:18:15.154673Z",
     "start_time": "2024-06-01T12:18:12.047657Z"
    }
   },
   "outputs": [],
   "source": [
    "sleep(3)\n",
    "profile = driver.find_element(By.XPATH,\"//*[@id='react-root']/div/div/div[2]/main/div/div/div/div/div/div[3]/section/div/div/div[1]/div/div/button/div/div[2]/div[1]/div[1]/div/div[1]/a/div/div[1]/span/span[1]\")\n",
    "profile.click() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87bfecea81d9ee46",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T12:18:19.345652Z",
     "start_time": "2024-06-01T12:18:16.319309Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "sleep(3)\n",
    "articles = driver.find_elements(By.XPATH,\"//article[@data-testid='tweet']\")\n",
    "length = len(articles)\n",
    "print(length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "510b68769dc6d1da",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T12:18:30.165598Z",
     "start_time": "2024-06-01T12:18:30.027737Z"
    }
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "#soup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d54bac33ce829864",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T12:18:30.936307Z",
     "start_time": "2024-06-01T12:18:30.932392Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "653\n",
      "530\n",
      "359\n"
     ]
    }
   ],
   "source": [
    "buttons = soup.find_all('button', {'data-testid': 'reply'})\n",
    "\n",
    "for button in buttons:\n",
    "    reply_text = button.find('span', {'class': 'css-1jxf684'}).text.strip()\n",
    "    print(reply_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ea6be797a3949ae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T12:18:36.634620Z",
     "start_time": "2024-06-01T12:18:36.628309Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "1300\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "mention = soup.find_all('button',{'data-testid': 'retweet'})\n",
    "\n",
    "for mention in mention:\n",
    "    reTweet = mention.find('span', {'class': 'css-1jxf684'}).text.strip()\n",
    "    if 'K' in reTweet:\n",
    "        retweet = int(float(reTweet.replace('K', '')) * 1000)\n",
    "    else:\n",
    "        retweet = int(reTweet)\n",
    "    print(retweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "daf8c53548af61ef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T12:18:40.847971Z",
     "start_time": "2024-06-01T12:18:40.842159Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet: Dive into the hues of my travels with this brand new collection from @AMtouristerIN \n",
      "Hashtags: #TravelWithAmericanTourister #AmericanTouristerIndia #AmericanTourister\n",
      "Tweet: Playing with a T-Rex? No biggie. \n",
      "Hashtags: @StayWrogn @Myntra  #WrognLikeABoss #MyntraSale #EndOfReasonSale #StayWrogn\n",
      "Tweet:  Out-of-this-world deals are here!\n",
      "Hashtags: @StayWrogn @Myntra @FlipKart  #WrognThrowawayFestival #ShopLikeABoss #AlienApproved #StayWrogn\n"
     ]
    }
   ],
   "source": [
    "tweets = soup.find_all('article', {'data-testid': 'tweet'})[:5]  # Limit to 5 tweets\n",
    "all_tweets = []\n",
    "\n",
    "for tweet in tweets:\n",
    "    try:\n",
    "        tweet_text_element = tweet.find('div', {'data-testid': 'tweetText'})\n",
    "        tweet_text = tweet_text_element.get_text() if tweet_text_element else \"N/A\"\n",
    "        split_text = tweet_text.split('\\n')\n",
    "        main_tweet = split_text[0] if split_text else \"N/A\"\n",
    "        hashtags_and_mentions = [part for part in split_text if part.startswith('#') or part.startswith('@')]\n",
    "        hashtags_and_mentions_str = ' '.join(hashtags_and_mentions)\n",
    "        print(\"Tweet:\", main_tweet)\n",
    "        print(\"Hashtags:\", hashtags_and_mentions_str)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing tweet: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "41ab5f7a98addf6f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T12:18:52.373661Z",
     "start_time": "2024-06-01T12:18:52.355179Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created At: 31/05/24 05:36\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from datetime import datetime\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "TimeStamp = driver.find_element(By.XPATH,\".//time\").get_attribute('datetime')\n",
    "dt_object = datetime.strptime(TimeStamp, \"%Y-%m-%dT%H:%M:%S.%fZ\")\n",
    "TimeStamp = dt_object.strftime(\"%d/%m/%y %H:%M\")\n",
    "print(\"Created At:\", TimeStamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8b557595a61df13e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T12:18:57.766827Z",
     "start_time": "2024-06-01T12:18:57.760580Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet: Dive into the hues of my travels with this brand new collection from @AMtouristerIN \n",
      "Hashtags: #TravelWithAmericanTourister #AmericanTouristerIndia #AmericanTourister\n",
      "Created At: 31/05/24 05:36\n",
      "Tweet: Playing with a T-Rex? No biggie. \n",
      "Hashtags: @StayWrogn @Myntra  #WrognLikeABoss #MyntraSale #EndOfReasonSale #StayWrogn\n",
      "Created At: 30/05/24 11:36\n",
      "Tweet:  Out-of-this-world deals are here!\n",
      "Hashtags: @StayWrogn @Myntra @FlipKart  #WrognThrowawayFestival #ShopLikeABoss #AlienApproved #StayWrogn\n",
      "Created At: 30/05/24 05:36\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import bs4\n",
    "\n",
    "tweets = soup.find_all('article', {'data-testid': 'tweet'})[:5]  # Limit to 5 tweets\n",
    "all_tweets = []\n",
    "\n",
    "for tweet in tweets:\n",
    "    try:\n",
    "        tweet_text_element = tweet.find('div', {'data-testid': 'tweetText'})\n",
    "        tweet_text = tweet_text_element.get_text() if tweet_text_element else \"N/A\"\n",
    "        split_text = tweet_text.split('\\n')\n",
    "        main_tweet = split_text[0] if split_text else \"N/A\"\n",
    "        hashtags_and_mentions = [part for part in split_text if part.startswith('#') or part.startswith('@')]\n",
    "        hashtags_and_mentions_str = ' '.join(hashtags_and_mentions)\n",
    "\n",
    "        created_at_element = tweet.find('time')\n",
    "        if created_at_element:\n",
    "            created_at_str = created_at_element['datetime']\n",
    "            created_at_datetime = datetime.strptime(created_at_str, '%Y-%m-%dT%H:%M:%S.%fZ')\n",
    "            created_at_formatted = created_at_datetime.strftime('%d/%m/%y %H:%M')\n",
    "\n",
    "        print(\"Tweet:\", main_tweet)\n",
    "        print(\"Hashtags:\", hashtags_and_mentions_str)\n",
    "        print(\"Created At:\", created_at_formatted)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing tweet: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f405dcb8d28d1ef4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T12:19:09.090799Z",
     "start_time": "2024-06-01T12:19:09.064220Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Follower Count: 63.3M\n"
     ]
    }
   ],
   "source": [
    "Followers = driver.find_element(By.XPATH, \"//a[contains(@href, '/verified_followers')]//span\").text\n",
    "print(\"Follower Count:\",Followers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3c62cb82795df9f4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T12:19:13.191858Z",
     "start_time": "2024-06-01T12:19:13.180157Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verified: True\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    verification_element = driver.find_element(By.XPATH, \"//button[@aria-label='Provides details about verified accounts.']\")\n",
    "    Verification = True\n",
    "except:\n",
    "    Verification = False\n",
    "print(\"Verified:\", Verification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "993164813fc5560e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T12:19:20.618014Z",
     "start_time": "2024-06-01T12:19:17.586635Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Username: imVkohli\n"
     ]
    }
   ],
   "source": [
    "sleep(3)\n",
    "# Extract the user tag\n",
    "user_tag = driver.find_element(By.XPATH, \".//div[@data-testid='User-Name']\").text\n",
    "\n",
    "# Split the text by newline\n",
    "split_text = user_tag.split('\\n')\n",
    "\n",
    "# Find the part that starts with '@' and remove the '@' symbol\n",
    "username = [part[1:] for part in split_text if part.startswith('@')][0]\n",
    "\n",
    "print(\"Username:\", username)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "92baa88916a413f6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-31T07:00:32.619832Z",
     "start_time": "2024-05-31T07:00:32.531143Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Followers' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 8\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcsv\u001b[39;00m\n\u001b[1;32m      3\u001b[0m tweet_data \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUsername\u001b[39m\u001b[38;5;124m'\u001b[39m: username,\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTweet\u001b[39m\u001b[38;5;124m'\u001b[39m: main_tweet,\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRetweet Count\u001b[39m\u001b[38;5;124m'\u001b[39m: retweet,\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMention Count\u001b[39m\u001b[38;5;124m'\u001b[39m: reply_text,\n\u001b[0;32m----> 8\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFollower Count\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[43mFollowers\u001b[49m,\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVerified\u001b[39m\u001b[38;5;124m'\u001b[39m: Verification,\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCreated At\u001b[39m\u001b[38;5;124m'\u001b[39m: created_at_formatted,\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHashtags\u001b[39m\u001b[38;5;124m'\u001b[39m: hashtags_and_mentions_str,\n\u001b[1;32m     12\u001b[0m }\n\u001b[1;32m     14\u001b[0m all_tweets\u001b[38;5;241m.\u001b[39mappend(tweet_data)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Specify all the fields to be included in the CSV file\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Followers' is not defined"
     ]
    }
   ],
   "source": [
    "# import csv\n",
    "# \n",
    "# tweet_data = {\n",
    "#     'Username': username,\n",
    "#     'Tweet': main_tweet,\n",
    "#     'Retweet Count': retweet,\n",
    "#     'Mention Count': reply_text,\n",
    "#     'Follower Count': Followers,\n",
    "#     'Verified': Verification,\n",
    "#     'Created At': created_at_formatted,\n",
    "#     'Hashtags': hashtags_and_mentions_str,\n",
    "# }\n",
    "# \n",
    "# all_tweets.append(tweet_data)\n",
    "# \n",
    "# # Specify all the fields to be included in the CSV file\n",
    "# fieldnames = [\n",
    "#     'Username', 'Tweet', 'Retweet Count', 'Mention Count',\n",
    "#     'Follower Count', 'Verified','Created At', 'Hashtags'\n",
    "# ]\n",
    "# \n",
    "# # Ensure all dictionaries have all the required fields with default values if missing\n",
    "# default_values = {field: '' for field in fieldnames}\n",
    "# \n",
    "# for tweet in all_tweets:\n",
    "#     for field in fieldnames:\n",
    "#         if field not in tweet:\n",
    "#             tweet[field] = default_values[field]\n",
    "# \n",
    "# # Save the extracted tweet data to a CSV file\n",
    "# with open('output.csv', 'w', newline='') as csvfile:\n",
    "#     writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "#     writer.writeheader()\n",
    "#     writer.writerows(all_tweets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c44988edeb85ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datetime import datetime\n",
    "# from bs4 import BeautifulSoup\n",
    "# from selenium import webdriver\n",
    "# from selenium.webdriver.common.by import By\n",
    "# \n",
    "# \n",
    "# \n",
    "# # Parse the page source with BeautifulSoup\n",
    "# soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "# \n",
    "# # Extract tweets\n",
    "# tweets = soup.find_all('article', {'data-testid': 'tweet'})[:5]  # Limit to 5 tweets\n",
    "# \n",
    "# for tweet in tweets:\n",
    "#     try:\n",
    "#         tweet_text_element = tweet.find('div', {'data-testid': 'tweetText'})\n",
    "#         tweet_text = tweet_text_element.get_text() if tweet_text_element else \"N/A\"\n",
    "#         split_text = tweet_text.split('\\n')\n",
    "#         main_tweet = split_text[0] if split_text else \"N/A\"\n",
    "#         hashtags_and_mentions = [part for part in split_text if part.startswith('#') or part.startswith('@')]\n",
    "#         hashtags_and_mentions_str = ' '.join(hashtags_and_mentions)\n",
    "# \n",
    "#         created_at_element = tweet.find('time')\n",
    "#         if created_at_element:\n",
    "#             created_at_str = created_at_element['datetime']\n",
    "#             created_at_datetime = datetime.strptime(created_at_str, '%Y-%m-%dT%H:%M:%S.%fZ')\n",
    "#             created_at_formatted = created_at_datetime.strftime('%d/%m/%y %H:%M')\n",
    "#         else:\n",
    "#             created_at_formatted = \"N/A\"\n",
    "# \n",
    "#         print(\"Tweet:\", main_tweet)\n",
    "#         print(\"Hashtags and Mentions:\", hashtags_and_mentions_str)\n",
    "#         print(\"Created At:\", created_at_formatted)\n",
    "# \n",
    "#     except Exception as e:\n",
    "#         print(f\"Error processing tweet: {e}\")\n",
    "# \n",
    "# # Extract reply counts\n",
    "# buttons = soup.find_all('button', {'data-testid': 'reply'})\n",
    "# for button in buttons:\n",
    "#     try:\n",
    "#         reply_text = button.find('span', {'class': 'css-1jxf684'}).text.strip()\n",
    "#         print(\"Reply Count:\", reply_text)\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error processing reply count: {e}\")\n",
    "# \n",
    "# # Extract retweet counts\n",
    "# mentions = soup.find_all('button', {'data-testid': 'retweet'})\n",
    "# for mention in mentions:\n",
    "#     try:\n",
    "#         reTweet = mention.find('span', {'class': 'css-1jxf684'}).text.strip()\n",
    "#         if 'K' in reTweet:\n",
    "#             retweet = int(float(reTweet.replace('K', '')) * 1000)\n",
    "#         else:\n",
    "#             retweet = int(reTweet)\n",
    "#         print(\"Retweet Count:\", retweet)\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error processing retweet count: {e}\")\n",
    "# \n",
    "# \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "caaa39d6af1627d1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-31T07:17:21.411597Z",
     "start_time": "2024-05-31T07:17:21.294303Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet: देश के युवाओं! \n",
      "Hashtags and Mentions: \n",
      "Created At: 07/03/24 11:04\n",
      "Tweet: आज प्रचार के आखिरी दिन मैं देश की महान जनता को प्रणाम करते हुए कांग्रेस के बब्बर शेर कार्यकर्ताओं से विश्वास के साथ कहना चाहता हूं कि INDIA की सरकार बनने जा रही है। \n",
      "Hashtags and Mentions: \n",
      "Created At: 30/05/24 15:08\n",
      "Tweet: कल शहीद अग्निवीर अजय सिंह के परिवार से मुलाकात हुई। \n",
      "Hashtags and Mentions: \n",
      "Created At: 30/05/24 12:28\n",
      "Tweet: नरेंद्र मोदी कहते हैं, \"मैं भगवान के लिए काम करता हूं\" और उनके चाटुकार कहते हैं - मोदी खुद भगवान हैं। \n",
      "Hashtags and Mentions: \n",
      "Created At: 30/05/24 09:57\n",
      "Reply Count: 14K\n",
      "Reply Count: 2.6K\n",
      "Reply Count: 572\n",
      "Reply Count: 1K\n",
      "Retweet Count: 35000\n",
      "Retweet Count: 14000\n",
      "Retweet Count: 8700\n",
      "Retweet Count: 8600\n"
     ]
    }
   ],
   "source": [
    "# from datetime import datetime\n",
    "# from bs4 import BeautifulSoup\n",
    "# from selenium import webdriver\n",
    "# from selenium.webdriver.common.by import By\n",
    "# \n",
    "# \n",
    "# \n",
    "# # Extract the user tag\n",
    "# try:\n",
    "#     user_tag = driver.find_element(By.XPATH, \".//div[@data-testid='User-Name']\").text\n",
    "#     split_text = user_tag.split('\\n')\n",
    "#     username = [part[1:] for part in split_text if part.startswith('@')][0]\n",
    "# except Exception as e:\n",
    "#     username = \"Error\"\n",
    "#     print(f\"Error extracting username: {e}\")\n",
    "#     \n",
    "# # Check verification status\n",
    "# try:\n",
    "#     verification_element = driver.find_element(By.XPATH, \"//button[@aria-label='Provides details about verified accounts.']\")\n",
    "#     verification = True\n",
    "# except:\n",
    "#     verification = False\n",
    "# \n",
    "# # Extract the follower count\n",
    "# try:\n",
    "#     followers = driver.find_element(By.XPATH, \"//a[contains(@href, '/verified_followers')]//span\").text\n",
    "# except Exception as e:\n",
    "#     followers = \"Error\"\n",
    "#     print(f\"Error extracting follower count: {e}\")\n",
    "# \n",
    "# # Parse the page source with BeautifulSoup\n",
    "# soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "# \n",
    "# # Extract tweets\n",
    "# tweets = soup.find_all('article', {'data-testid': 'tweet'})[:5]  # Limit to 5 tweets\n",
    "# \n",
    "# for tweet in tweets:\n",
    "#     try:\n",
    "#         tweet_text_element = tweet.find('div', {'data-testid': 'tweetText'})\n",
    "#         tweet_text = tweet_text_element.get_text() if tweet_text_element else \"N/A\"\n",
    "#         split_text = tweet_text.split('\\n')\n",
    "#         main_tweet = split_text[0] if split_text else \"N/A\"\n",
    "#         hashtags_and_mentions = [part for part in split_text if part.startswith('#') or part.startswith('@')]\n",
    "#         hashtags_and_mentions_str = ' '.join(hashtags_and_mentions)\n",
    "# \n",
    "#         created_at_element = tweet.find('time')\n",
    "#         if created_at_element:\n",
    "#             created_at_str = created_at_element['datetime']\n",
    "#             created_at_datetime = datetime.strptime(created_at_str, '%Y-%m-%dT%H:%M:%S.%fZ')\n",
    "#             created_at_formatted = created_at_datetime.strftime('%d/%m/%y %H:%M')\n",
    "#         else:\n",
    "#             created_at_formatted = \"N/A\"\n",
    "# \n",
    "#         print(\"Tweet:\", main_tweet)\n",
    "#         print(\"Hashtags and Mentions:\", hashtags_and_mentions_str)\n",
    "#         print(\"Created At:\", created_at_formatted)\n",
    "# \n",
    "#     except Exception as e:\n",
    "#         print(f\"Error processing tweet: {e}\")\n",
    "# \n",
    "# # Extract reply counts\n",
    "# buttons = soup.find_all('button', {'data-testid': 'reply'})\n",
    "# for button in buttons:\n",
    "#     try:\n",
    "#         reply_text = button.find('span', {'class': 'css-1jxf684'}).text.strip()\n",
    "#         print(\"Reply Count:\", reply_text)\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error processing reply count: {e}\")\n",
    "# \n",
    "# # Extract retweet counts\n",
    "# mentions = soup.find_all('button', {'data-testid': 'retweet'})\n",
    "# for mention in mentions:\n",
    "#     try:\n",
    "#         reTweet = mention.find('span', {'class': 'css-1jxf684'}).text.strip()\n",
    "#         if 'K' in reTweet:\n",
    "#             retweet = int(float(reTweet.replace('K', '')) * 1000)\n",
    "#         else:\n",
    "#             retweet = int(reTweet)\n",
    "#         print(\"Retweet Count:\", retweet)\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error processing retweet count: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ae23ba8db1fdd35b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T12:28:15.034746Z",
     "start_time": "2024-06-01T12:28:15.025710Z"
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unterminated string literal (detected at line 82) (3861053430.py, line 82)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[26], line 82\u001b[0;36m\u001b[0m\n\u001b[0;31m    df = pd.DataFrame(tweet_data, columns=['Username', 'Tweet', 'Retweet Count', 'Reply Count', 'F\u001b[0m\n\u001b[0m                                                                                                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unterminated string literal (detected at line 82)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from time import sleep\n",
    "\n",
    "# Wait for the page to load\n",
    "sleep(3)\n",
    "\n",
    "# Extract the follower count\n",
    "try:\n",
    "    followers = driver.find_element(By.XPATH, \"//a[contains(@href, '/verified_followers')]//span\").text\n",
    "except Exception as e:\n",
    "    followers = \"Error\"\n",
    "    print(f\"Error extracting follower count: {e}\")\n",
    "\n",
    "# Check verification status\n",
    "try:\n",
    "    verification_element = driver.find_element(By.XPATH, \"//button[@aria-label='Provides details about verified accounts.']\")\n",
    "    verification = True\n",
    "except:\n",
    "    verification = False\n",
    "\n",
    "# Extract the user tag\n",
    "try:\n",
    "    user_tag = driver.find_element(By.XPATH, \".//div[@data-testid='User-Name']\").text\n",
    "    split_text = user_tag.split('\\n')\n",
    "    username = [part[1:] for part in split_text if part.startswith('@')][0]\n",
    "except Exception as e:\n",
    "    username = \"Error\"\n",
    "    print(f\"Error extracting username: {e}\")\n",
    "\n",
    "# Parse the page source with BeautifulSoup\n",
    "soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "# Extract tweets\n",
    "tweets = soup.find_all('article', {'data-testid': 'tweet'})[:5]  # Limit to 5 tweets\n",
    "\n",
    "tweet_data = []\n",
    "for tweet in tweets:\n",
    "    try:\n",
    "        tweet_text_element = tweet.find('div', {'data-testid': 'tweetText'})\n",
    "        tweet_text = tweet_text_element.get_text() if tweet_text_element else \"N/A\"\n",
    "        split_text = tweet_text.split('\\n')\n",
    "        main_tweet = split_text[0] if split_text else \"N/A\"\n",
    "        hashtags_and_mentions = [part for part in split_text if part.startswith('#') or part.startswith('@')]\n",
    "        hashtags_and_mentions_str = ' '.join(hashtags_and_mentions)\n",
    "\n",
    "        created_at_element = tweet.find('time')\n",
    "        if created_at_element:\n",
    "            created_at_str = created_at_element['datetime']\n",
    "            created_at_datetime = datetime.strptime(created_at_str, '%Y-%m-%dT%H:%M:%S.%fZ')\n",
    "            created_at_formatted = created_at_datetime.strftime('%d/%m/%y %H:%M')\n",
    "        else:\n",
    "            created_at_formatted = \"N/A\"\n",
    "\n",
    "        # Extract reply counts\n",
    "        try:\n",
    "            reply_text = tweet.find('div', {'data-testid': 'reply'}).text.strip()\n",
    "        except Exception as e:\n",
    "            reply_text = \"Error\"\n",
    "            print(f\"Error processing reply count: {e}\")\n",
    "\n",
    "        # Extract retweet counts\n",
    "        try:\n",
    "            retweet_text = tweet.find('div', {'data-testid': 'retweet'}).text.strip()\n",
    "            if 'K' in retweet_text:\n",
    "                retweet = int(float(retweet_text.replace('K', '')) * 1000)\n",
    "            else:\n",
    "                retweet = int(retweet_text)\n",
    "        except Exception as e:\n",
    "            retweet = \"Error\"\n",
    "            print(f\"Error processing retweet count: {e}\")\n",
    "\n",
    "        tweet_data.append([username, main_tweet, retweet, reply_text, followers, verification, created_at_formatted, hashtags_and_mentions_str])\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing tweet: {e}\")\n",
    "\n",
    "# Save the data to a CSV file using pandas\n",
    "df = pd.DataFrame(tweet_data, columns=['Username', 'Tweet', 'Retweet Count', 'Reply Count', 'F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b99c624462a814",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T12:28:01.901620Z",
     "start_time": "2024-06-01T12:28:01.894075Z"
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (815647329.py, line 17)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[23], line 17\u001b[0;36m\u001b[0m\n\u001b[0;31m    except FileNotFoundError:\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "\n",
    "buttons = soup.find_all('button', {'data-testid': 'reply'})\n",
    "\n",
    "for button in buttons:\n",
    "    reply_text = button.find('span', {'class': 'css-1jxf684'}).text.strip()\n",
    "    print(reply_text)\n",
    "\n",
    "# Open the CSV file for writing with error handling\n",
    "try:\n",
    "  with open('reply_texts.csv', 'w', newline='') as csvfile:  # 'w' for write mode (creates new file if needed)\n",
    "    writer = csv.writer(csvfile)\n",
    "    # Write header row (optional)\n",
    "    writer.writerow([\"Reply Text\"])\n",
    "\n",
    "  except FileNotFoundError:\n",
    "    print(\"Error: Could not create the CSV file.\")\n",
    "  else:\n",
    "    # Loop through buttons and write to CSV\n",
    "    for button in buttons:\n",
    "      reply_text = button.find('span', {'class': 'css-1jxf684'}).text.strip()\n",
    "      writer.writerow([reply_text])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d557a6d9a670e671",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T12:28:04.925446Z",
     "start_time": "2024-06-01T12:28:04.920522Z"
    }
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<string>, line 19)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m<string>:19\u001b[0;36m\u001b[0m\n\u001b[0;31m    else:\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "buttons = soup.find_all('button', {'data-testid': 'reply'})\n",
    "\n",
    "for button in buttons:\n",
    "    reply_text = button.find('span', {'class': 'css-1jxf684'}).text.strip()\n",
    "    print(reply_text)\n",
    "\n",
    "\n",
    "# Open the CSV file for writing with error handling\n",
    "try:\n",
    "  with open('reply_texts.csv', 'w', newline='') as csvfile:  # 'w' for write mode (creates new file if needed)\n",
    "    writer = csv.writer(csvfile)\n",
    "    # Write header row (optional)\n",
    "    writer.writerow([\"Reply Text\"])\n",
    "\n",
    "except FileNotFoundError:  # Removed extra space before except\n",
    "    print(\"Error: Could not create the CSV file.\")\n",
    "  else:\n",
    "    # Loop through buttons and write to CSV\n",
    "    for button in buttons:\n",
    "      reply_text = button.find('span', {'class': 'css-1jxf684'}).text.strip()\n",
    "      writer.writerow([reply_text])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7113611ccf07eb45",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
